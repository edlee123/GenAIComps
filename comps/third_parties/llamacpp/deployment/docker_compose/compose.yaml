# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

services:
  llamacpp-server:
    image: ghcr.io/ggerganov/llama.cpp:server-b4419
    container_name: llamacpp-server
    ports:
      - ${LLM_ENDPOINT_PORT:-8008}:80
    volumes:
      # Download the .gguf models to this path.
      - ${MODEL_PATH:-~/models}:/models
    environment:
      LOGFLAG: False
      no_proxy: ${no_proxy}
      https_proxy: ${http_proxy}
      http_proxy: ${https_proxy}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      LLM_ENDPOINT_PORT: ${LLM_ENDPOINT_PORT}
      host_ip: ${host_ip}
      # llama.cpp env variables. Please refer to reference:
      # https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md
      LLAMA_ARG_PORT: 80
      LLAMA_ARG_MODEL: /$LLM_MODEL_ID
      LLAMA_ARG_CTX_SIZE: ${LLAMA_ARG_CTX_SIZE:-4096}
      LLAMA_ARG_N_PARALLEL: 2
      LLAMA_ARG_ENDPOINT_METRICS: 1
    ipc: host
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://${host_ip}:${LLM_ENDPOINT_PORT}/health || exit 1" ]
      interval: 10s
      timeout: 10s
      retries: 100

networks:
  default:
    driver: bridge
